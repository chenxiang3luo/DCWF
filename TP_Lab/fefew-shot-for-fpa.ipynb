{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f7d077",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-15T07:32:34.505997Z",
     "iopub.status.busy": "2023-01-15T07:32:34.505153Z",
     "iopub.status.idle": "2023-01-15T07:32:34.515866Z",
     "shell.execute_reply": "2023-01-15T07:32:34.515021Z"
    },
    "papermill": {
     "duration": 0.017532,
     "end_time": "2023-01-15T07:32:34.517925",
     "exception": false,
     "start_time": "2023-01-15T07:32:34.500393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9048d67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:32:34.524301Z",
     "iopub.status.busy": "2023-01-15T07:32:34.523999Z",
     "iopub.status.idle": "2023-01-15T07:32:39.372469Z",
     "shell.execute_reply": "2023-01-15T07:32:39.371366Z"
    },
    "papermill": {
     "duration": 4.855181,
     "end_time": "2023-01-15T07:32:39.375171",
     "exception": false,
     "start_time": "2023-01-15T07:32:34.519990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 18:25:56.807241: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-22 18:25:57.986884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 18:25:58.856726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22310 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9c:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# The model is the DF model by Sirinam et al\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import ELU\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "import tensorflow as tf\n",
    "import keras.layers as KL\n",
    "import keras.models as KM\n",
    "from keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = False  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "\n",
    "def DF(input_shape=None, emb_size=None):\n",
    "    # -----------------Entry flow -----------------\n",
    "    input_data = Input(shape=input_shape)\n",
    "\n",
    "    filter_num = ['None', 32, 64, 128, 256]\n",
    "    kernel_size = ['None', 8, 8, 8, 8]\n",
    "    conv_stride_size = ['None', 1, 1, 1, 1]\n",
    "    pool_stride_size = ['None', 4, 4, 4, 4]\n",
    "    pool_size = ['None', 8, 8, 8, 8]\n",
    "\n",
    "    model = Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
    "                   strides=conv_stride_size[1], padding='same', name='block1_conv1')(input_data)\n",
    "    model = ELU(alpha=1.0, name='block1_adv_act1')(model)\n",
    "\n",
    "    model = Conv1D(filters=filter_num[1], kernel_size=kernel_size[1],\n",
    "                   strides=conv_stride_size[1], padding='same', name='block1_conv2')(model)\n",
    "    model = ELU(alpha=1.0, name='block1_adv_act2')(model)\n",
    "    model = MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
    "                         padding='same', name='block1_pool')(model)\n",
    "    model = Dropout(0.1, name='block1_dropout')(model)\n",
    "\n",
    "    model = Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
    "                   strides=conv_stride_size[2], padding='same', name='block2_conv1')(model)\n",
    "    model = Activation('relu', name='block2_act1')(model)\n",
    "    model = Conv1D(filters=filter_num[2], kernel_size=kernel_size[2],\n",
    "                   strides=conv_stride_size[2], padding='same', name='block2_conv2')(model)\n",
    "    model = Activation('relu', name='block2_act2')(model)\n",
    "    model = MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[3],\n",
    "                         padding='same', name='block2_pool')(model)\n",
    "    model = Dropout(0.1, name='block2_dropout')(model)\n",
    "\n",
    "    model = Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
    "                   strides=conv_stride_size[3], padding='same', name='block3_conv1')(model)\n",
    "    model = Activation('relu', name='block3_act1')(model)\n",
    "    model = Conv1D(filters=filter_num[3], kernel_size=kernel_size[3],\n",
    "                   strides=conv_stride_size[3], padding='same', name='block3_conv2')(model)\n",
    "    model = Activation('relu', name='block3_act2')(model)\n",
    "    model = MaxPooling1D(pool_size=pool_size[3], strides=pool_stride_size[3],\n",
    "                         padding='same', name='block3_pool')(model)\n",
    "    model = Dropout(0.1, name='block3_dropout')(model)\n",
    "\n",
    "    model = Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
    "                   strides=conv_stride_size[4], padding='same', name='block4_conv1')(model)\n",
    "    model = Activation('relu', name='block4_act1')(model)\n",
    "    model = Conv1D(filters=filter_num[4], kernel_size=kernel_size[4],\n",
    "                   strides=conv_stride_size[4], padding='same', name='block4_conv2')(model)\n",
    "    model = Activation('relu', name='block4_act2')(model)\n",
    "    model = MaxPooling1D(pool_size=pool_size[4], strides=pool_stride_size[4],\n",
    "                         padding='same', name='block4_pool')(model)\n",
    "\n",
    "    output = Flatten()(model)\n",
    "\n",
    "    dense_layer = Dense(emb_size, name='FeaturesVec')(output)\n",
    "    shared_conv2 = Model(inputs=input_data, outputs=dense_layer)\n",
    "    return shared_conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f8f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:32:39.381313Z",
     "iopub.status.busy": "2023-01-15T07:32:39.380795Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 3024.963706,
     "end_time": "2023-01-15T08:23:04.341191",
     "exception": false,
     "start_time": "2023-01-15T07:32:39.377485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dataset/dataset/extracted_AWF775/\n",
      "with parameters, Alpha: 0.1, Batch_size: 128, Embedded_size: 64, Epoch_num: 30\n",
      "Triplet_Model\n",
      "number of classes: 775\n",
      "Load traces with  (19375, 5000, 1)\n",
      "Total size allocated on RAM :  96.875 MB\n",
      "X_train Anchor:  (232500,)\n",
      "X_train Positive:  (232500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 18:26:00.861912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22310 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:9c:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " anchor (InputLayer)            [(None, 5000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " positive (InputLayer)          [(None, 5000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " negative (InputLayer)          [(None, 5000, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 64)           1369344     ['anchor[0][0]',                 \n",
      "                                                                  'positive[0][0]',               \n",
      "                                                                  'negative[0][0]']               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 1)            0           ['model[0][0]',                  \n",
      "                                                                  'model[2][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                ()                   0           ['dot[0][0]',                    \n",
      "                                                                  'dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,369,344\n",
      "Trainable params: 1,369,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "built new hard generator for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3137/3399004279.py:266: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_triplet.fit_generator(generator=gen_hard.next_train(),\n",
      "2023-01-22 18:26:04.395513: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2023-01-22 18:26:07.033416: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816/1816 [==============================] - 122s 63ms/step - loss: 0.0166\n",
      "606/606 [==============================] - 2s 3ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 1\n",
      "1816/1816 [==============================] - 290s 160ms/step - loss: 0.0433\n",
      "606/606 [==============================] - 2s 3ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 2\n",
      "1816/1816 [==============================] - 263s 145ms/step - loss: 0.0523\n",
      "606/606 [==============================] - 2s 3ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 3\n",
      "1816/1816 [==============================] - 244s 134ms/step - loss: 0.0586\n",
      "606/606 [==============================] - 2s 3ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 4\n",
      "1816/1816 [==============================] - 244s 134ms/step - loss: 0.0552\n",
      "606/606 [==============================] - 1s 2ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 5\n",
      "1816/1816 [==============================] - 234s 129ms/step - loss: 0.0544\n",
      "606/606 [==============================] - 2s 3ms/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "built new hard generator for epoch 6\n",
      "1679/1816 [==========================>...] - ETA: 17s - loss: 0.0500"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dot\n",
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras.backend import set_session\n",
    "import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = False  # to log device placement (on which device the operation ran)\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "# This is tuned hyper-parameters\n",
    "alpha = 0.1\n",
    "batch_size_value = 128\n",
    "emb_size = 64\n",
    "number_epoch = 30\n",
    "\n",
    "description = 'Triplet_Model'\n",
    "Training_Data_PATH = '/root/dataset/dataset/extracted_AWF775/' \n",
    "print(Training_Data_PATH)\n",
    "print(\"with parameters, Alpha: %s, Batch_size: %s, Embedded_size: %s, Epoch_num: %s\"%(alpha, batch_size_value, emb_size, number_epoch))\n",
    "\n",
    "\n",
    "alpha_value = float(alpha)\n",
    "print(description)\n",
    "\n",
    "# ================================================================================\n",
    "# This part is to prepare the files' index for geenrating triplet examples\n",
    "# and formulating each epoch inputs\n",
    "\n",
    "# Extract all folders' names\n",
    "dirs = sorted(os.listdir(Training_Data_PATH))\n",
    "\n",
    "# Each given folder name (URL of each class), we assign class id\n",
    "# e.g. {'adp.com' : 23, ...}\n",
    "name_to_classid = {d:i for i,d in enumerate(dirs)}\n",
    "\n",
    "# Just reverse from previous step\n",
    "# Each given class id, show the folder name (URL of each class)\n",
    "# e.g. {23 : 'adp.com', ...}\n",
    "classid_to_name = {v:k for k,v in name_to_classid.items()}\n",
    "\n",
    "num_classes = len(name_to_classid)\n",
    "print(\"number of classes: \"+str(num_classes))\n",
    "\n",
    "# Each directory, there are n traces corresponding to the identity\n",
    "# We map each trace path with an integer id, then build dictionaries\n",
    "# We are mapping\n",
    "#   path_to_id and id_to_path\n",
    "#   classid_to_ids and id_to_classid\n",
    "\n",
    "# read all directories\n",
    "# c is class\n",
    "# name_to_classid.items() contains [(directory, classid), ('slickdeals.net', 547), ...]\n",
    "\n",
    "trace_paths = {c:[directory + \"/\" + img for img in sorted(os.listdir(Training_Data_PATH + directory))]\n",
    "         for directory,c in name_to_classid.items()}\n",
    "# trace_paths --> {0: ['104.com.tw/104.com.tw_0001.pkl', '104.com.tw/104.com.tw_0002.pkl',...] ,....}\n",
    "\n",
    "# retreive all traces\n",
    "# to create the list of all traces paths\n",
    "all_traces_path = []\n",
    "for trace_list in trace_paths.values():\n",
    "    all_traces_path += trace_list\n",
    "# all_trace_path --> ['104.com.tw/104.com.tw_0001.pkl', '104.com.tw/104.com.tw_0002.pkl',...]\n",
    "# len(all_trace_path = num_class * num_examples e.g. 700 * 25\n",
    "\n",
    "# map to integers\n",
    "# just map each path to sequence of ID (from 1 to len(all_trace_path)\n",
    "path_to_id = {v: k for k, v in enumerate(all_traces_path)}\n",
    "# path_to_id --> {'chron.com/chron.com_0006.pkl': 1185, 'habrahabr.ru/habrahabr.ru_0001.pkl': 2680, ...}\n",
    "id_to_path = {v: k for k, v in path_to_id.items()}\n",
    "# id_to_path --> {0: '104.com.tw/104.com.tw_0001.pkl', 1: '104.com.tw/104.com.tw_0002.pkl', ...}\n",
    "\n",
    "# build mapping between traces and class\n",
    "classid_to_ids = {k: [path_to_id[path] for path in v] for k, v in trace_paths.items()}\n",
    "# classid_to_ids --> {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 1: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],...]\n",
    "id_to_classid = {v: c for c, traces in classid_to_ids.items() for v in traces}\n",
    "# id_to_classid --> {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 1,...]\n",
    "\n",
    "# open trace\n",
    "all_traces = []\n",
    "for path in id_to_path.values():\n",
    "    each_path = Training_Data_PATH + path\n",
    "    with open(each_path, 'rb') as handle:\n",
    "        each_trace = pickle.load(handle,encoding='iso-8859-1')\n",
    "    all_traces += [each_trace]\n",
    "\n",
    "all_traces = np.vstack((all_traces))\n",
    "all_traces = all_traces[:, :, np.newaxis]\n",
    "print(\"Load traces with \",all_traces.shape)\n",
    "print(\"Total size allocated on RAM : \", str(all_traces.nbytes / 1e6) + ' MB')\n",
    "\n",
    "def build_pos_pairs_for_id(classid): # classid --> e.g. 0\n",
    "    traces = classid_to_ids[classid]\n",
    "    # pos_pairs is actually the combination C(10,2)\n",
    "    # e.g. if we have 10 example [0,1,2,...,9]\n",
    "    # and want to create a pair [a, b], where (a, b) are different and order does not matter\n",
    "    # e.g. [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9),\n",
    "    # (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9)...]\n",
    "    # C(10, 2) = 45\n",
    "    pos_pairs = [(traces[i], traces[j]) for i in range(len(traces)) for j in range(i+1, len(traces))]\n",
    "    random.shuffle(pos_pairs)\n",
    "    return pos_pairs\n",
    "\n",
    "def build_positive_pairs(class_id_range):\n",
    "    # class_id_range = range(0, num_classes)\n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    for class_id in class_id_range:\n",
    "        pos = build_pos_pairs_for_id(class_id)\n",
    "        # -- pos [(1, 9), (0, 9), (3, 9), (4, 8), (1, 4),...] --> (anchor example, positive example)\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]] # identity\n",
    "            listX2 += [pair[1]] # positive example\n",
    "    perm = np.random.permutation(len(listX1))\n",
    "    # random.permutation([1,2,3]) --> [2,1,3] just random\n",
    "    # random.permutation(5) --> [1,0,4,3,2]\n",
    "    # In this case, we just create the random index\n",
    "    # Then return pairs of (identity, positive example)\n",
    "    # that each element in pairs in term of its index is randomly ordered.\n",
    "    return np.array(listX1)[perm], np.array(listX2)[perm]\n",
    "\n",
    "Xa_train, Xp_train = build_positive_pairs(range(0, num_classes))\n",
    "\n",
    "# Gather the ids of all network traces that are used for training\n",
    "# This just union of two sets set(A) | set(B)\n",
    "all_traces_train_idx = list(set(Xa_train) | set(Xp_train))\n",
    "print(\"X_train Anchor: \", Xa_train.shape)\n",
    "print(\"X_train Positive: \", Xp_train.shape)\n",
    "\n",
    "# Build a loss which doesn't take into account the y_true, as# Build\n",
    "# we'll be passing only 0\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "# The real loss is here\n",
    "def cosine_triplet_loss(X):\n",
    "    _alpha = alpha_value\n",
    "    positive_sim, negative_sim = X\n",
    "\n",
    "    losses = K.maximum(0.0, negative_sim - positive_sim + _alpha)\n",
    "    return K.mean(losses)\n",
    "\n",
    "# ------------------- Hard Triplet Mining -----------\n",
    "# Naive way to compute all similarities between all network traces.\n",
    "\n",
    "def build_similarities(conv, all_imgs):\n",
    "    embs = conv.predict(all_imgs)\n",
    "    embs = embs / np.linalg.norm(embs, axis=-1, keepdims=True)\n",
    "    all_sims = np.dot(embs, embs.T)\n",
    "    return all_sims\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def build_negatives(anc_idxs, pos_idxs, similarities, neg_imgs_idx, num_retries=50):\n",
    "    # If no similarities were computed, return a random negative\n",
    "    if similarities is None:\n",
    "        return random.sample(neg_imgs_idx,len(anc_idxs))\n",
    "    final_neg = []\n",
    "    # for each positive pair\n",
    "    for (anc_idx, pos_idx) in zip(anc_idxs, pos_idxs):\n",
    "        anchor_class = id_to_classid[anc_idx]\n",
    "        #positive similarity\n",
    "        sim = similarities[anc_idx, pos_idx]\n",
    "        # find all negatives which are semi(hard)\n",
    "        possible_ids = np.where((similarities[anc_idx] + alpha_value) > sim)[0]\n",
    "        possible_ids = intersect(neg_imgs_idx, possible_ids)\n",
    "        appended = False\n",
    "        for iteration in range(num_retries):\n",
    "            if len(possible_ids) == 0:\n",
    "                break\n",
    "            idx_neg = random.choice(possible_ids)\n",
    "            if id_to_classid[idx_neg] != anchor_class:\n",
    "                final_neg.append(idx_neg)\n",
    "                appended = True\n",
    "                break\n",
    "        if not appended:\n",
    "            final_neg.append(random.choice(neg_imgs_idx))\n",
    "    return final_neg\n",
    "\n",
    "\n",
    "class SemiHardTripletGenerator():\n",
    "    def __init__(self, Xa_train, Xp_train, batch_size, all_traces, neg_traces_idx, conv):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.traces = all_traces\n",
    "        self.Xa = Xa_train\n",
    "        self.Xp = Xp_train\n",
    "        self.cur_train_index = 0\n",
    "        self.num_samples = Xa_train.shape[0]\n",
    "        self.neg_traces_idx = neg_traces_idx\n",
    "        self.all_anchors = list(set(Xa_train))\n",
    "        self.mapping_pos = {v: k for k, v in enumerate(self.all_anchors)}\n",
    "        self.mapping_neg = {k: v for k, v in enumerate(self.neg_traces_idx)}\n",
    "        if conv:\n",
    "            self.similarities = build_similarities(conv, self.traces)\n",
    "        else:\n",
    "            self.similarities = None\n",
    "\n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index = 0\n",
    "\n",
    "            # fill one batch\n",
    "            traces_a = self.Xa[self.cur_train_index:self.cur_train_index + self.batch_size]\n",
    "            traces_p = self.Xp[self.cur_train_index:self.cur_train_index + self.batch_size]\n",
    "            traces_n = build_negatives(traces_a, traces_p, self.similarities, self.neg_traces_idx)\n",
    "\n",
    "            yield ([self.traces[traces_a],\n",
    "                    self.traces[traces_p],\n",
    "                    self.traces[traces_n]],\n",
    "                   np.zeros(shape=(traces_a.shape[0]))\n",
    "                   )\n",
    "\n",
    "# Training the Triplet Model\n",
    "shared_conv2 = DF(input_shape=(5000,1), emb_size=emb_size)\n",
    "\n",
    "anchor = Input((5000, 1), name='anchor')\n",
    "positive = Input((5000, 1), name='positive')\n",
    "negative = Input((5000, 1), name='negative')\n",
    "\n",
    "a = shared_conv2(anchor)\n",
    "p = shared_conv2(positive)\n",
    "n = shared_conv2(negative)\n",
    "\n",
    "# The Dot layer in Keras now supports built-in Cosine similarity using the normalize = True parameter.\n",
    "# From the Keras Docs:\n",
    "# keras.layers.Dot(axes, normalize=True)\n",
    "# normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product.\n",
    "#  If set to True, then the output of the dot product is the cosine proximity between the two samples.\n",
    "pos_sim = Dot(axes=-1, normalize=True)([a,p])\n",
    "neg_sim = Dot(axes=-1, normalize=True)([a,n])\n",
    "\n",
    "# customized loss\n",
    "loss = Lambda(cosine_triplet_loss,\n",
    "              output_shape=(1,))(\n",
    "             [pos_sim,neg_sim])\n",
    "\n",
    "model_triplet = Model(\n",
    "    inputs=[anchor, positive, negative],\n",
    "    outputs=loss)\n",
    "print(model_triplet.summary())\n",
    "\n",
    "opt = optimizers.gradient_descent_v2.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_triplet.compile(loss=identity_loss, optimizer=opt)\n",
    "\n",
    "batch_size = batch_size_value\n",
    "# At first epoch we don't generate hard triplets\n",
    "gen_hard = SemiHardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx, None)\n",
    "nb_epochs = number_epoch\n",
    "csv_logger = CSVLogger('/root/Lab/TP_Lab/log/Training_Log_%s.csv'%description, append=True, separator=';')\n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"built new hard generator for epoch \"+str(epoch)) #update the fit_generator\n",
    "    model_triplet.fit_generator(generator=gen_hard.next_train(),\n",
    "                    steps_per_epoch=Xa_train.shape[0] // batch_size,\n",
    "                    epochs=1, verbose=1, callbacks=[csv_logger])\n",
    "    gen_hard = SemiHardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx, shared_conv2)\n",
    "    #For no semi-hard_triplet\n",
    "    #gen_hard = HardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx, None)\n",
    "    shared_conv2.save('/root/Lab/TP_Lab/trained_model/%s.h5'%description)\n",
    "shared_conv2.save('/root/Lab/TP_Lab/trained_model/%s.h5'%description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393525f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3038.286484,
   "end_time": "2023-01-15T08:23:05.145010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-15T07:32:26.858526",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
